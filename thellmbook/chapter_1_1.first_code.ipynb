{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59668673-024d-4cdd-8dc2-04a71c8c9592",
   "metadata": {},
   "source": [
    "### Let's Get Started ###\n",
    "> When you use an LLM, two models are loaded:\n",
    ">1. The generative model itself\n",
    ">2. The underlying tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226cb64a-5d3b-4904-bae8-109d24952ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6905183731f480995b06d6812dd8749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"cuda\",\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867b3fc-5901-4f99-992e-30976c8e2c8c",
   "metadata": {},
   "source": [
    "- Use pipeline package from transformers to encapsulate model, tokenizer and text generation process in to a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f77321-6f6f-42ee-8cc9-703c41bbf3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9ae11-ba59-4ae5-aa11-7b86273f1b29",
   "metadata": {},
   "source": [
    "### Generate firt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62d46ce8-f4ed-4d3f-983b-c87b36ed9cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['role', 'content'])\n"
     ]
    }
   ],
   "source": [
    "# The promt (user input / query)\n",
    "messages = [\n",
    "    {\"role\":\"user\",\n",
    "     \"content\":\"create a funny joke about chickens.!\"}\n",
    "]\n",
    "print(messages[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "887d20ea-0b14-4374-8080-0b22fe24601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db753c10-e7ce-4485-99ad-c2dfdc65c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken join the band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "source": [
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f28ff-c434-44e7-8252-2f64146eeb37",
   "metadata": {},
   "source": [
    "### That it is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f6368-a50d-4150-aa63-20ffe07bc42b",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "1. In this first chapter of the book, we delved into the revolutionary impact LLMs have had on the Language AI field. It has significantly changed our approach to tasks such as translation, classification, summarization, and more. Through a recent history of Language AI, we explored the fundamentals of several types of LLMs, from a simple bag-of-words representation to more complex representations using neural networks.\n",
    "2. We discussed the attention mechanism as a step toward encoding context within models, a vital component of what makes LLMs so capable. We touched on two main categories of models that use this incredible mechanism: representation models (encoder-only) like BERT and generative models (decoder-only) like the GPT family of models. Both categories are considered large language models throughout this book.\n",
    "3. Overall, the chapter provided an overview of the landscape of Language AI, including its applications, societal and ethical implications, and the resources needed to run such models. We ended by generating our first text using Phi-3, a model that will be used throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2745e1a-9c2b-466d-ab85-124844ac15f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
