{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367deaab-b65f-47d5-a30a-3e59fe6c79bc",
   "metadata": {},
   "source": [
    "## Cheptar 3\n",
    "\n",
    "### Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623f48d6-989f-4007-b058-adacfb62fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf5569a-8963-4307-8bd8-9b96c9f313ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b8a07ca6374c6fafe5446ad173b3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load model and tokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            device_map = \"cuda\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec976ea0-54c0-49d3-9dbc-7e8be81ea8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crerate a pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = False,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a0ae3-cbcd-4e98-aede-270f02b3bca0",
   "metadata": {},
   "source": [
    "### autoregressive models\n",
    "> (e.g., the model’s first generated token is used to generate the second token). \n",
    "> They’re called autoregressive models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09c8d8c1-a1fb-4436-bc69-6fb1268c6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write an email apoligizing to Sarah for the tragicgardening mishap. Explain how it happen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d588f3-fdf3-4d65-9fe0-1851c2d7ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "996fef83-3df1-48d3-9a63-8e39070dbb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", express sincere regret, and offer to help her replant the flowers.\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I am deeply sorry for the unfortunate incident that occurred in your garden. I understand how much effort and love you put into nurturing your plants, and it pains me to know that I have caused such distress.\n",
      "\n",
      "The incident happened when I was trying to help you with the watering system. Unfortunately, I accidentally knocked over the water\n"
     ]
    }
   ],
   "source": [
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27f2f2e8-983a-451d-9510-5ebda3388b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "          (rotary_emb): Phi3RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm()\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffa72a5d-bc3f-4092-ab8c-8655959f168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "\n",
    "## Tokenize the input prompt\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors = \"pt\").input_ids\n",
    "\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "## Get output from the model before LM head\n",
    "model_output = model.model(input_ids)\n",
    "\n",
    "## get output of LM head\n",
    "lm_head_output = model.lm_head(model_output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f60f36d5-2a0b-4699-bd9d-ecf7e465a060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9839b8e-7d49-479a-b1d3-1ee03e0ebf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32064])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108501c-8b58-4f3d-b173-efbfe6a5ee02",
   "metadata": {},
   "source": [
    "### KV cache \"Technique to speed up the processing\"\n",
    "'''Recall that when generating the second token, we simply\n",
    " append the output token to the input and do another\n",
    " forward pass through the model. If we give the model the\n",
    " ability to cache the results of the previous calculation\n",
    " (especially some of the specific vectors in the attention\n",
    " mechanism), we no longer need to repeat the calculations\n",
    " of the previous streams. This time the only needed\n",
    " calculation is for the last stream. This is an optimization\n",
    " technique called the keys and values (kv) cache and it'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "926f6aa1-d6e5-4b01-9386-25976f53e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-347 ns ± 313 ns per loop (mean ± std. dev. of 7 runs, -1 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n -1\n",
    "##generate the text\n",
    "generation_output = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    max_new_tokens = 100,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd558a81-ae05-49ab-93e7-6ae76bcc31ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 s ± 353 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ff8f0-f2cc-4372-894d-3acd08dce3bd",
   "metadata": {},
   "source": [
    "### Inside the Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebf057-3a61-4c03-b4a8-7d9f3d21c921",
   "metadata": {},
   "source": [
    "### Type of Attentions\n",
    "> 1. Self Attention\n",
    "> 2. Multi head attention\n",
    "> 3. Multuquery Attention\n",
    "> 4. Grouped Query Attention\n",
    "> 5. Flash Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e254b70-675d-4811-8ec8-d8fc943eec11",
   "metadata": {},
   "source": [
    "## Summary\n",
    " '''In this chapter we discussed the main intuitions of\n",
    " Transformers and recent developments that enable the\n",
    " latest Transformer LLMs. We went over many new\n",
    " concepts, so let’s break down the key concepts that we\n",
    " discussed in this chapter:'''\n",
    "- A Transformer LLM generates one token at a time.\n",
    "- That output token is appended to the prompt, then this updated prompt is presented to the model again for another forward pass to generate the next token.\n",
    "- The three major components of the Transformer LLM are the tokenizer, a stack of Transformer blocks, and a language modeling head.\n",
    "- The tokenizer contains the token vocabulary for the model. The model has token embeddings associated with those tokens. Breaking the text into tokens and then using the embeddings of these tokens is the first step in the token generation process.\n",
    "- The forward pass flows through all the stages once, one by one.\n",
    "- Near the end of the process, the LM head scores the probabilities of the next possible token. Decoding strategies inform which actual token to pick as the output for this generation step (sometimes it’s the most probable next token, but not always).\n",
    "- One reason the Transformer excels is its ability to process tokens in parallel. Each of the input tokens flow into their individual tracks or streams of processing. The number of streams is the model’s “context size” and this represents the max number of tokens the model can operate on.\n",
    "- Because Transformer LLMs loop to generate the text one token at a time, it’s a good idea to cache the processing results of each step so we don’t duplicate the processing effort (these results are stored as various matrices within the layers).\n",
    "- The majority of processing happens within Transformer blocks. These are made up of two components. One of them is the feedforward neural network, which is able to store information and make predictions and interpolations from data it was trained on.\n",
    "- The second major component of a Transformer block is the attention layer. Attention incorporates contextual information to allow the model to better capture the nuance of language.\n",
    "- Attention happens in two major steps: (1) scoring relevance and (2) combining information.\n",
    "- A Transformer attention layer conducts several attention operations in parallel, each occurring inside an attention head, and their outputs are aggregated to make up the output of the attention layer.\n",
    "- Attention can be accelerated via sharing the keys and values matrices between all heads, or groups of heads (grouped-query attention).\n",
    "- Methods like Flash Attention speed up the attention calculation by optimizing how the operation is done on the different memory systems of a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f8a76-d8dd-489d-bba2-69631918c9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
