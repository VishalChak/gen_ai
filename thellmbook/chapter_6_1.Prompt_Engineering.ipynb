{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c9c93-854b-4a65-bc1b-2c465001eb4c",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "> Through prompt engineering, we can design these prompts in a way that enhances the quality of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b1f9be-1c56-45b4-b6fb-1385c3f52403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load a text generation model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8319f3a-6d7b-417f-a198-289c172f8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85841d59c2ec45e1a92f338237bd46bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### load model and tokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            device_map=\"cuda\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc117bc1-1c77-4455-baf0-61d73f6ae121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae67ba6c-cee4-4478-ae66-823f733e99f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "               model=model,\n",
    "               tokenizer=tokenizer,\n",
    "               return_full_text=False,\n",
    "               max_new_tokens=500,\n",
    "                do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25979a4a-67c3-4bd2-a8b7-62b5d6098c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken join the band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "source": [
    "###   prompt\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"write a joke about chickens\"}\n",
    "]\n",
    "\n",
    "### Generate the output\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebd240-b65d-471f-b81c-e82040c04923",
   "metadata": {},
   "source": [
    "### Chat Templates\n",
    "> Transformer pipeline first convert our messages in to specific chat templets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cc3415-0643-410c-bfb5-3b7cd5875b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "write a joke about chickens<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27b0134-67cb-42b8-87a1-cd00d876268c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken go to the séance?\n",
      "\n",
      "\n",
      "To get to the other side!\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, do_sample=True, temperature=1)\n",
    "print(output[0][\"generated_text\"])\n",
    "\n",
    "### Note that every time you rerun this piece of code, the output will change! temperature introduces stochastic behavior since the model now randomly selects tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a989ce4-6651-41a9-962b-60ad50212183",
   "metadata": {},
   "outputs": [],
   "source": [
    "### top_k parameter controls exactly how many tokens the LLM can consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59a1e53-6055-4a36-ba43-a261de7093ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why do chickens never argue about the meaning of words? Because even when debating, they know how to cluck and peck their way to the essentials instead of complicating things with grammar and semantics!\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, do_sample=True, top_p=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee9152-c58d-479c-8c2b-97482ce6c1e8",
   "metadata": {},
   "source": [
    "### Usecase and value of Temperature and top_p\n",
    "\n",
    "| Example use case | Temperature | top_p |Description |\n",
    "|:--------:|:--------:|:--------:|:--------:|\n",
    "|  Brainstorming session   |  High  |  High   | High randomness with large pool of potential tokens. The results will be highly diverse, often leading to very creative and unexpected results   |\n",
    "|  Email generation        |  Low   |  Low   |Deterministic output with high probable predicted tokens. This results in predictable, focused, and conservative outputs.    |\n",
    "|  Creative writing        |  High   |  Low   |High randomness with a small pool of  potential tokens. This combination produces creative outputs but still remains coherent.    |\n",
    "| Translation              |  Low   | High   | Deterministic output with high probable predicted tokens. Produces coherent output with a wider range of vocabulary, leading to outputs with linguistic variety   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1606291-2210-4af0-af0f-5dc9df3f7d1f",
   "metadata": {},
   "source": [
    "### Intro to prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033aa93-126f-4eac-ad82-de141c1c2feb",
   "metadata": {},
   "source": [
    "###### Instruction-Based Prompting \n",
    "> prompting is often used to have the LLM answer a specific question or resolve a certain task. This is referred to as instruction-based prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cecb0-c4f5-4fdd-a6ee-a8181de2bc8c",
   "metadata": {},
   "source": [
    "#### Component for good Prompt\n",
    "#### Basic componernts:\n",
    "> 1. Specificity: Accurately describe what you want to achieve. Instead of asking the LLM to “Write a description for a product” ask it to “Write a description for a product in less than two sentences and use a formal tone.”\n",
    "\n",
    "> 2. Hallucination: LLMs may generate incorrect information confidently,which is referred to as hallucination. To reduce its impact, we can ask the LLM to only generate an answer if it knows  the answer. If it does not know the answer, it can respond with “I don’t know.”\n",
    "\n",
    "> 3. Order: Either begin or end your prompt with the instruction. Especially with long prompts, information in the middle is often forgotten.1 LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect).\n",
    "\n",
    "\n",
    "####  Advanced Prompt Engineering\n",
    "\n",
    "> 1. Persona: Describe what role the LLM should take on. For example, use  “You are an expert in astrophysics” if you want to ask a question about astrophysics\n",
    "> 2. Instruction: The task itself. Make sure this is as specific as possible. We do not want to leave much room for interpretation.\n",
    "> 3. Context: Additional information describing the context of the problem or task. It answers questions like “What is the reason for the instruction?”\n",
    "> 4. Format: The format the LLM should use to output the generated text. Without it, the LLM will come up with a format itself, which is troublesome in automated systems.\n",
    "> 5. Audience: The target of the generated text. This also describes the level of the generated output. For education purposes, it is often helpful to use ELI5 (“Explain it like I’m 5”).\n",
    "> 6. Tone: he tone of voice the LLM should use in the generated text. If you are writing a formal email to your boss, you might not want to use an informal tone of voice.\n",
    "> 7. Data: The main data related to the task itself\n",
    "\n",
    "\n",
    "##### Prompt components Examples :\n",
    "> 1. persona = \"You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.\n",
    "> 2. instruction = \"Summarize the key findings of the paper provided.\n",
    "> 3. context = \"Your summary should extract the most crucial points  that can help researchers quickly understand the most vital information of the paper\n",
    "> 4. data_format = \"Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.\n",
    "> 5. audience = \"The summary is designed for busy researchers that  quickly need to grasp the newest trends in Large Language  Models.\n",
    "> 6. tone = \"The tone should be professional and clear.\n",
    "> 7. data = f\"Text to summarize: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb7709-de89-4e87-9308-862021aa46b5",
   "metadata": {},
   "source": [
    "#### In-Context Learning: Providing Examples\n",
    "\n",
    "> This comes in a number of forms depending on how many examples you show the LLM.\n",
    "> 1. Zero-shot prompting does not leverage examples,\n",
    "> 2.  one-shot prompts use a single example, and\n",
    "> 3.  few-shot prompts use two or more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d2ae2e7-c888-4513-a01b-d383701539b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "A 'Gigamuru' is a type of Japanese musical  instrument. An example of a sentence that uses the word Gigamuru is:<|end|>\n",
      "<|assistant|>\n",
      "I have a Gigamuru that my uncle gave me as a  gift. I love to play it at home.<|end|>\n",
      "<|user|>\n",
      "To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = [ { \"role\": \"user\", \"content\": \"A 'Gigamuru' is a type of Japanese musical  instrument. An example of a sentence that uses the word Gigamuru is:\"}, \n",
    "                { \"role\": \"assistant\", \"content\": \"I have a Gigamuru that my uncle gave me as a  gift. I love to play it at home.\"},\n",
    "                    { \"role\": \"user\",\"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\"}\n",
    "                   ]\n",
    "print(tokenizer.apply_chat_template(one_shot_prompt, tokenize= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a8a23b-5595-46d0-9580-69dd42dd9244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why do chickens never argue about the meaning of words? Because even when debating, they know how to cluck and peck their way to the essentials instead of complicating things with grammar and semantics!\n"
     ]
    }
   ],
   "source": [
    "## Generate the output\n",
    "outputs = pipe(one_shot_prompt)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c12644-8034-4e3c-a354-7b8357008cd4",
   "metadata": {},
   "source": [
    "####  Chain Prompting: Breaking up the Problem.\n",
    "\n",
    "> let us say we want to use an LLM to create a product name, slogan, and sales pitch for us based on a number of product features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "325dfa0a-d749-4249-9b3c-b72d236edef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name: ChatSage\n",
      "Slogan: \"Unleashing the power of AI to enhance your conversations.\"\n"
     ]
    }
   ],
   "source": [
    " # Create name and slogan for a product\n",
    "\n",
    "product_prompt = [ {\"role\": \"user\",\n",
    "                    \"content\": \"Create a name and slogan for a  chatbot that leverages LLMs.\"} ]\n",
    "outputs = pipe(product_prompt)\n",
    "product_description = outputs[0][\"generated_text\"]\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd92a3-5e8b-47d2-a74f-188b8d1aaf9f",
   "metadata": {},
   "source": [
    "> we can use the generated output as input for the LLM to generate a sales pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e60330bc-bbb2-45d5-8cb4-75342da2ce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Introducing ChatSage, the ultimate AI companion that revolutionizes your conversations. With our cutting-edge technology, we unleash the power of AI to enhance your interactions, making every conversation more engaging and meaningful. Experience the future of communication with ChatSage today!\n"
     ]
    }
   ],
   "source": [
    " # Based on a name and slogan for a product, generate a sales  pitch\n",
    "sales_prompt = [ {\"role\": \"user\", \"content\": f\"Generate a very short sales pitch for the following product: ' { product_description }'\"}]\n",
    "outputs = pipe(sales_prompt)\n",
    "sales_pitch = outputs[0][\"generated_text\"]\n",
    "print(sales_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aee3ed-9d14-485d-9bfb-80431072169c",
   "metadata": {},
   "source": [
    "####  Reasoning with Generative Models\n",
    "\n",
    "##### Chain of thought prompt engineraring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2721fa87-f3bf-44c3-b36f-52c09517aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cafeteria started with 23 apples. They used 20 apples for lunch, so they had 23 - 20 = 3 apples left. After buying 6 more apples, they now have 3 + 6 = 9 apples. The answer is 9.\n"
     ]
    }
   ],
   "source": [
    " ### Answering with chain-of-thought\n",
    "cot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\"},\n",
    "    {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"}\n",
    "]\n",
    " # Generate the output\n",
    "outputs = pipe(cot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438da64-4269-47f2-8157-9c62c6de6c3a",
   "metadata": {},
   "source": [
    "##### Zero-shot chain-of-thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08fe3b02-76b1-477a-9271-4e7d17bffddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: The cafeteria starts with 23 apples.\n",
      "Step 2: They used 20 apples to make lunch, so we subtract 20 from the initial amount: 23 - 20 = 3 apples remaining.\n",
      "Step 3: The cafeteria bought 6 more apples, so we add 6 to the remaining amount: 3 + 6 = 9 apples.\n",
      "\n",
      "The cafeteria now has 9 apples.\n"
     ]
    }
   ],
   "source": [
    " # Zero-shot chain-of-thought\n",
    "zeroshot_cot_prompt = [ {\"role\": \"user\",\n",
    "                         \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let's think step-by-step.\"}\n",
    "                      ]\n",
    "\n",
    "outputs = pipe(zeroshot_cot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fa9f5-ddcf-4fdb-859a-e3aff72a77c2",
   "metadata": {},
   "source": [
    "#### Zero-shot tree-of-thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9797dde5-2d65-48cb-80b1-0279bbeda047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Expert 1:\n",
      "Step 1: Start with the initial number of apples, which is 23.\n",
      "\n",
      "Expert 2:\n",
      "Step 1: Subtract the number of apples used for lunch, which is 20.\n",
      "Step 2: Add the number of apples bought, which is 6.\n",
      "\n",
      "Expert 3:\n",
      "Step 1: Start with the initial number of apples, which is 23.\n",
      "Step 2: Subtract the number of apples used for lunch, which is 20.\n",
      "Step 3: Add the number of apples bought, which is 6.\n",
      "\n",
      "Results:\n",
      "All three experts arrived at the same answer:\n",
      "\n",
      "Expert 1: 23 - 20 + 6 = 9 apples\n",
      "Expert 2: (23 - 20) + 6 = 9 apples\n",
      "Expert 3: (23 - 20) + 6 = 9 apples\n",
      "\n",
      "All three experts agree that the cafeteria has 9 apples left.\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot tree-of-thought\n",
    "zeroshot_tot_prompt = [{\"role\": \"user\",\n",
    "                        \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results.\"\n",
    "                       }]\n",
    "\n",
    " # Generate the output\n",
    "outputs = pipe(zeroshot_tot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a972b9-5ab3-4848-9992-66a4b485fb6d",
   "metadata": {},
   "source": [
    "#### llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a910ac7-29aa-4b5c-95a0-eed005bf5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Phi-3\n",
    "\n",
    "# from llama_cpp.llama import Llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc5634-f8c5-42ed-894c-66eeedb0c8f0",
   "metadata": {},
   "source": [
    "### Summary\n",
    "> 1. In this chapter, we explored the basics of using generative models through prompt engineering and output verification. We focused on the creativity and potential complexity that comes with prompt engineering. These components of a prompt are key in generating and optimizing output appropriate for different use cases.\n",
    "> 2. We further explored advanced prompt engineering techniques such as in-context learning and chain-of thought. These methods involve guiding generative models to reason through complex problems by providing examples or phrases that encourage step-by-step thinking thereby mimicking human reasoning processes.\n",
    "> 3. Overall, this chapter demonstrated that prompt engineering is a crucial aspect of working with LLMs, as it allows us to effectively communicate our needs and preferences to the model. By mastering prompt engineering techniques, we can unlock some of the potential of LLMs and generate high-quality responses that meet our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3dea65-9334-449d-9217-13ab88c49e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
