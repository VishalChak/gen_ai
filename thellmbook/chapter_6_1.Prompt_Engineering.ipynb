{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c9c93-854b-4a65-bc1b-2c465001eb4c",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "> Through prompt engineering, we can design these prompts in a way that enhances the quality of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b1f9be-1c56-45b4-b6fb-1385c3f52403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load a text generation model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8319f3a-6d7b-417f-a198-289c172f8359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50421eee3d5c405cab2d00c762fbe00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### load model and tokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            device_map=\"cuda\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc117bc1-1c77-4455-baf0-61d73f6ae121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec43214329544f492a1d8872498ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a72f7031be426d9a7615480e1c6250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3498ccb4d52442989ed41d975c6312f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8241d5b9f7304097880e560d24e5718c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39ce53dadd74ed7b5593f308d6d0e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae67ba6c-cee4-4478-ae66-823f733e99f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "               model=model,\n",
    "               tokenizer=tokenizer,\n",
    "               return_full_text=False,\n",
    "               max_new_tokens=500,\n",
    "                do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25979a4a-67c3-4bd2-a8b7-62b5d6098c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken join the band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "source": [
    "###   prompt\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"write a joke about chickens\"}\n",
    "]\n",
    "\n",
    "### Generate the output\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebd240-b65d-471f-b81c-e82040c04923",
   "metadata": {},
   "source": [
    "### Chat Templates\n",
    "> Transformer pipeline first convert our messages in to specific chat templets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95cc3415-0643-410c-bfb5-3b7cd5875b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "write a joke about chickens<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27b0134-67cb-42b8-87a1-cd00d876268c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken go to the séance? To get to the next level of egg-sistance!\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, do_sample=True, temperature=1)\n",
    "print(output[0][\"generated_text\"])\n",
    "\n",
    "### Note that every time you rerun this piece of code, the output will change! temperature introduces stochastic behavior since the model now randomly selects tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a989ce4-6651-41a9-962b-60ad50212183",
   "metadata": {},
   "outputs": [],
   "source": [
    "### top_k parameter controls exactly how many tokens the LLM can consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b59a1e53-6055-4a36-ba43-a261de7093ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why don't secret agents get chickens for spies?\n",
      "\n",
      "\n",
      "Because even a hen needs her feathers, and spies know that no operation can go chicken if you're spying \"eggs-ample\"!\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, do_sample=True, top_p=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee9152-c58d-479c-8c2b-97482ce6c1e8",
   "metadata": {},
   "source": [
    "### Usecase and value of Temperature and top_p\n",
    "\n",
    "| Example use case | Temperature | top_p |Description |\n",
    "|:--------:|:--------:|:--------:|:--------:|\n",
    "|  Brainstorming session   |  High  |  High   | High randomness with large pool of potential tokens. The results will be highly diverse, often leading to very creative and unexpected results   |\n",
    "|  Email generation        |  Low   |  Low   |Deterministic output with high probable predicted tokens. This results in predictable, focused, and conservative outputs.    |\n",
    "|  Creative writing        |  High   |  Low   |High randomness with a small pool of  potential tokens. This combination produces creative outputs but still remains coherent.    |\n",
    "| Translation              |  Low   | High   | Deterministic output with high probable predicted tokens. Produces coherent output with a wider range of vocabulary, leading to outputs with linguistic variety   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1606291-2210-4af0-af0f-5dc9df3f7d1f",
   "metadata": {},
   "source": [
    "### Intro to prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033aa93-126f-4eac-ad82-de141c1c2feb",
   "metadata": {},
   "source": [
    "###### Instruction-Based Prompting \n",
    "> prompting is often used to have the LLM answer a specific question or resolve a certain task. This is referred to as instruction-based prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cecb0-c4f5-4fdd-a6ee-a8181de2bc8c",
   "metadata": {},
   "source": [
    "#### Component for good Prompt\n",
    "#### Basic componernts:\n",
    "> 1. Specificity: Accurately describe what you want to achieve. Instead of asking the LLM to “Write a description for a product” ask it to “Write a description for a product in less than two sentences and use a formal tone.”\n",
    "\n",
    "> 2. Hallucination: LLMs may generate incorrect information confidently,which is referred to as hallucination. To reduce its impact, we can ask the LLM to only generate an answer if it knows  the answer. If it does not know the answer, it can respond with “I don’t know.”\n",
    "\n",
    "> 3. Order: Either begin or end your prompt with the instruction. Especially with long prompts, information in the middle is often forgotten.1 LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect).\n",
    "\n",
    "\n",
    "####  Advanced Prompt Engineering\n",
    "\n",
    "> 1. Persona: Describe what role the LLM should take on. For example, use  “You are an expert in astrophysics” if you want to ask a question about astrophysics\n",
    "> 2. Instruction: The task itself. Make sure this is as specific as possible. We do not want to leave much room for interpretation.\n",
    "> 3. Context: Additional information describing the context of the problem or task. It answers questions like “What is the reason for the instruction?”\n",
    "> 4. Format: The format the LLM should use to output the generated text. Without it, the LLM will come up with a format itself, which is troublesome in automated systems.\n",
    "> 5. Audience: The target of the generated text. This also describes the level of the generated output. For education purposes, it is often helpful to use ELI5 (“Explain it like I’m 5”).\n",
    "> 6. Tone: he tone of voice the LLM should use in the generated text. If you are writing a formal email to your boss, you might not want to use an informal tone of voice.\n",
    "> 7. Data: The main data related to the task itself\n",
    "\n",
    "\n",
    "##### Prompt components Examples :\n",
    "> 1. persona = \"You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.\n",
    "> 2. instruction = \"Summarize the key findings of the paper provided.\n",
    "> 3. context = \"Your summary should extract the most crucial points  that can help researchers quickly understand the most vital information of the paper\n",
    "> 4. data_format = \"Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.\n",
    "> 5. audience = \"The summary is designed for busy researchers that  quickly need to grasp the newest trends in Large Language  Models.\n",
    "> 6. tone = \"The tone should be professional and clear.\n",
    "> 7. data = f\"Text to summarize: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb7709-de89-4e87-9308-862021aa46b5",
   "metadata": {},
   "source": [
    "#### In-Context Learning: Providing Examples\n",
    "\n",
    "> This comes in a number of forms depending on how many examples you show the LLM.\n",
    "> 1. Zero-shot prompting does not leverage examples,\n",
    "> 2.  one-shot prompts use a single example, and\n",
    "> 3.  few-shot prompts use two or more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d2ae2e7-c888-4513-a01b-d383701539b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "A 'Gigamuru' is a type of Japanese musical  instrument. An example of a sentence that uses the word Gigamuru is:<|end|>\n",
      "<|assistant|>\n",
      "I have a Gigamuru that my uncle gave me as a  gift. I love to play it at home.<|end|>\n",
      "<|user|>\n",
      "To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = [ { \"role\": \"user\", \"content\": \"A 'Gigamuru' is a type of Japanese musical  instrument. An example of a sentence that uses the word Gigamuru is:\"}, \n",
    "                { \"role\": \"assistant\", \"content\": \"I have a Gigamuru that my uncle gave me as a  gift. I love to play it at home.\"},\n",
    "                    { \"role\": \"user\",\"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\"}\n",
    "                   ]\n",
    "print(tokenizer.apply_chat_template(one_shot_prompt, tokenize= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09a8a23b-5595-46d0-9580-69dd42dd9244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why don't secret agents get chickens for spies?\n",
      "\n",
      "\n",
      "Because even a hen needs her feathers, and spies know that no operation can go chicken if you're spying \"eggs-ample\"!\n"
     ]
    }
   ],
   "source": [
    "## Generate the output\n",
    "outputs = pipe(one_shot_prompt)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c12644-8034-4e3c-a354-7b8357008cd4",
   "metadata": {},
   "source": [
    "####  Chain Prompting: Breaking up the Problem.\n",
    "\n",
    "> let us say we want to use an LLM to create a product name, slogan, and sales pitch for us based on a number of product features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325dfa0a-d749-4249-9b3c-b72d236edef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name: ChatSage\n",
      "Slogan: \"Unleashing the power of AI to enhance your conversations.\"\n"
     ]
    }
   ],
   "source": [
    " # Create name and slogan for a product\n",
    "\n",
    "product_prompt = [ {\"role\": \"user\",\n",
    "                    \"content\": \"Create a name and slogan for a  chatbot that leverages LLMs.\"} ]\n",
    "outputs = pipe(product_prompt)\n",
    "product_description = outputs[0][\"generated_text\"]\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd92a3-5e8b-47d2-a74f-188b8d1aaf9f",
   "metadata": {},
   "source": [
    "> we can use the generated output as input for the LLM to generate a sales pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e60330bc-bbb2-45d5-8cb4-75342da2ce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Introducing ChatSage, the ultimate AI companion that revolutionizes your conversations. With our cutting-edge technology, we unleash the power of AI to enhance your interactions, making every conversation more engaging and meaningful. Experience the future of communication with ChatSage today!\n"
     ]
    }
   ],
   "source": [
    " # Based on a name and slogan for a product, generate a sales  pitch\n",
    "sales_prompt = [ {\"role\": \"user\", \"content\": f\"Generate a very short sales pitch for the following product: ' { product_description }'\"}]\n",
    "outputs = pipe(sales_prompt)\n",
    "sales_pitch = outputs[0][\"generated_text\"]\n",
    "print(sales_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aee3ed-9d14-485d-9bfb-80431072169c",
   "metadata": {},
   "source": [
    "####  Reasoning with Generative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721fa87-f3bf-44c3-b36f-52c09517aefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe3b02-76b1-477a-9271-4e7d17bffddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
